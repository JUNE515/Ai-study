1.	Learning rate에 대해서 설명을 먼저 하자면 learning rate가 너무 크다면 학습이 제대로 이루어지지 않고, learning rate가 너무 작다면 학습은 잘 이루어지지만 너무 느리기 때문에 효율적이지 못하다. 
  따라서 우리는 그 중간에 최적의 점을 찾으려고 노력한다. 따라서 learning rate를 고정하기보다 learning rate의 값을 바꿔간다면 더 좋은 결과를 낼 수 있다.


1)	LAMBDA LR: 시간에 따라서 가중치를 점점 감소시키는 방법이다. Lambda식 (0.65**epoch)을 초기의 값에 곱하여 새로운 learning rate를 만들어가는 방식이다. 
  따라서 처음에는 보다 큰 값으로 효율적으로 학습하고 나중에는 점점 learning rate가 작아지며 천천히 잘 학습할 수 있다. 


2)	Cyclical LR: 아까랑은 조금 다른 개념으로 주기성을 갖는 learning rate를 사용하는 것이다. 여기서도 종류가 있는데 learning rate가 감소하는 동시에 주기성을 갖고 있는 것이 있고, 그냥 주기성을 갖고 있는 것이 있다. 
  주기성을 이용하는 이유는 낮은 것이 local minimum으로 가는 것에는 적합하지만, 큰 것이 그 local인 곳을 탈출하여 좀 더 global minimum으로 갈 수 있는 가능성이 있기 때문이다. 


2.	내가 생각할 때 과적합을 방지하기 위해서는 앙상블을 이용하는 것도 하나의 방법이다.앙상블중에 하나로 bagging을 이용하는 것이다. Train 데이터셋을 다양한 k개의 데이터를 가진 데이터셋을 몇 개 만들어서 
  그 각각의 데이터셋을 모델을 돌린 결과의 평균을 결과로 채택하는 것이다. 물론 bias에는 도움이 되지는 않지만 결국 variance에는 도움이 된다. 데이터셋을 독립적이라 가정하고 나누어서 진행한다면 분산이 적어지고, 
분산이 적다는 뜻은 다른 데이터셋으로 진행했을 때의 편차가 적다는 뜻이다. 결국 좀 더 각각의 데이터의 차이에 대해 robust하다는 뜻이다. 따라서 데이터셋을 독립적으로 나누는 방법이 더 좋을 수 있다.
  
또한 validation set을 따로 추가하여서 일정 이상 학습이 진행됨에도 불구하고 train error는 줄어들지만 validation set의 error가 줄어들지 않는다면 학습을 멈추며 과적합되지 않게 멈추는 방법도 있다. 
